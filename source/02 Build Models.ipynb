{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.2.0\nNum GPUs Available:  1\n"
    }
   ],
   "source": [
    "# cell magic functions :\n",
    "#   `%%capture` blocks jupyter notebook output,\n",
    "#   `%%script false --no-raise-error` avoid cell execution\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta #Date arithmetic\n",
    "import mplfinance as mpf\n",
    "\n",
    "#import PIL\n",
    "#print('Pillow Version:', PIL.__version__)\n",
    "\n",
    "# load and show an image with Pillow\n",
    "from PIL import Image\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Below 4 lines needed to reload function definitions\n",
    "import sys, importlib\n",
    "from project_functions import *\n",
    "importlib.reload(sys.modules['project_functions'])\n",
    "from project_functions import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_x,set_y = readXYfromDisk()\n",
    "\n",
    "print(set_x.shape)\n",
    "print(set_y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Test Single file \n",
    "#file = h5py.File(\"data/Set0_128x128_Window20.h5\", \"r\")\n",
    "file = h5py.File(\"data/Archive_256x256_Window20/SetA2M.AX.h5\", \"r\")\n",
    "set_x = file[\"set_x\"][:]\n",
    "set_y = file[\"set_y\"][:]\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_x = set_x.reshape((set_x.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(12362, 256, 256, 3)\n(12362,)\n"
    }
   ],
   "source": [
    "#Normalise X\n",
    "set_x = set_x/255.0\n",
    "\n",
    "#One hot encode Y\n",
    "#set_y = tf.keras.utils.to_categorical(set_y)\n",
    "\n",
    "print(set_x.shape)\n",
    "print(set_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dtype('uint8')"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "set_x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1 2]\n[2736 1025 1127]\n"
    }
   ],
   "source": [
    "values, counts = np.unique(set_y, axis=0, return_counts=True)\n",
    "print(values)\n",
    "print(counts)\n",
    "\n",
    "# into a dict for presentation\n",
    "#{tuple(a):b for a,b in zip(values, counts)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(11661, 64, 64, 3)\n(11661,)\n"
    }
   ],
   "source": [
    "print(set_x.shape)\n",
    "print(set_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "51911"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# Alternate train test split\n",
    "\n",
    "m = set_x.shape[0]                  # number of training examples\n",
    "\n",
    "#train_pct_index = int(0.8 * len(X))\n",
    "train_pct_index = np.random.randint(m, size= (m * 80)//100)\n",
    "\n",
    "#train_set_x, test_set_x = set_x[train_pct_index,:,:,:], set_x[~train_pct_index:,:,:,:]\n",
    "#train_set_y, test_set_y = set_y[:train_pct_index], set_y[~train_pct_index:]\n",
    "\n",
    "len(train_pct_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "64889\n[ 3078 28215  9127 ... 12096 53025 48458]\n[45534 32335 35195 ... 34386 32356 35895]\n(48666, 64, 64, 3)\n(48666, 3)\n(16222, 64, 64, 3)\n(16222, 3)\n"
    }
   ],
   "source": [
    "m = set_x.shape[0]                  # number of training examples\n",
    "\n",
    "#Shuffle X and Y ?\n",
    "#permutation = list(np.random.permutation(m))\n",
    "#set_x = set_x[permutation,:,:,:]\n",
    "#set_y = set_y[permutation,:]\n",
    "    \n",
    "print(m)\n",
    "\n",
    "#Split train/test\n",
    "training_idx = np.random.randint(m, size= (m * 75)//100)\n",
    "test_idx = np.random.randint(m, size= (m * 25)//100)\n",
    "print(training_idx)\n",
    "print(test_idx)\n",
    "\n",
    "train_set_x, test_set_x = set_x[training_idx,:,:,:], set_x[test_idx,:,:,:]\n",
    "train_set_y, test_set_y = set_y[training_idx], set_y[test_idx]\n",
    "\n",
    "\n",
    "print(train_set_x.shape)\n",
    "print(train_set_y.shape)\n",
    "print(test_set_x.shape)\n",
    "print(test_set_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 196608)            0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               50331904  \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 387       \n=================================================================\nTotal params: 50,365,187\nTrainable params: 50,365,187\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    #keras.layers.Flatten(input_shape=(288, 432,3)),\n",
    "    #keras.layers.Flatten(batch_input_shape=(512,64,64,3)),\n",
    "    keras.layers.Flatten(input_shape=(256,256,3)),\n",
    "    keras.layers.Dense(256, activation='relu'),            \n",
    "    keras.layers.Dense(128, activation='relu'),        \n",
    "    keras.layers.Dense(3,activation='softmax')\n",
    "])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/1000\n387/387 [==============================] - 7s 17ms/step - loss: 1.0405 - accuracy: 0.4991\nEpoch 2/1000\n387/387 [==============================] - 7s 17ms/step - loss: 1.0405 - accuracy: 0.4991\nEpoch 3/1000\n387/387 [==============================] - 7s 17ms/step - loss: 1.0405 - accuracy: 0.4991\nEpoch 4/1000\n387/387 [==============================] - 7s 17ms/step - loss: 1.0405 - accuracy: 0.4991\nEpoch 5/1000\n387/387 [==============================] - 6s 17ms/step - loss: 1.0404 - accuracy: 0.4991\nEpoch 6/1000\n387/387 [==============================] - 7s 17ms/step - loss: 1.0404 - accuracy: 0.4991\nEpoch 7/1000\n387/387 [==============================] - 7s 17ms/step - loss: 1.0404 - accuracy: 0.4991\nEpoch 8/1000\n223/387 [================>.............] - ETA: 2s - loss: 1.0373 - accuracy: 0.5035"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a3ae9a316fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#model.fit(train_set_x, train_set_y, epochs=10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#,validation_split=0.0001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m--> 847\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     if (self._delta_t_batch > 0. and\n\u001b[1;32m    303\u001b[0m         delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \"\"\"\n\u001b[1;32m   3494\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3495\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3497\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3401\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3403\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3404\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3515\u001b[0m         \u001b[0mkth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3516\u001b[0m     \u001b[0;31m# Check if the array contains any nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3517\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3518\u001b[0m         \u001b[0mkth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \"\"\"\n\u001b[0;32m--> 392\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adamax(learning_rate=1e-5),\n",
    "              #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              #loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#model.fit(train_set_x, train_set_y, epochs=10)\n",
    "history = model.fit(set_x, set_y, batch_size = 32,epochs=1000,verbose=1) #,validation_split=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dc29ec3c47c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set the vertical range to [ 0 - 1 ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    " \n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5)) \n",
    "plt.grid(True) \n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [ 0 - 1 ]\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "33/33 [==============================] - 0s 2ms/step - loss: 1.0056 - accuracy: 0.5458\n"
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_set_x,  test_set_y, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pythonprogramming.net/loading-custom-data-deep-learning-python-tensorflow-keras/\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in image_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "#print(X[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\n",
    "\n",
    "print(X[0])\n",
    "print(y[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.pluralsight.com/guides/importing-image-data-into-numpy-arrays\n",
    "#https://realpython.com/storing-images-in-python/\n",
    "\n",
    "import PIL\n",
    "print('Pillow Version:', PIL.__version__)\n",
    "\n",
    "# load and show an image with Pillow\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the image form working directory\n",
    "image = Image.open('data/images/fig5.png')\n",
    "# summarize some details about the image\n",
    "print(image.format)\n",
    "print(image.size)\n",
    "print(image.mode)\n",
    "# show the image\n",
    "#load_image.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "\n",
    "# convert image to numpy array\n",
    "data = asarray(image)\n",
    "print(type(data))\n",
    "# summarize shape\n",
    "print(data.shape)\n",
    "\n",
    "#image_without_alpha = image[:,:,:3]\n",
    "#print(data[:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image resize\n",
    "load_img_rz = np.array(Image.open('data/images/fig5.png').resize((200,200)))\n",
    "Image.fromarray(load_img_rz).save('data/images/r_kolala.png')\n",
    "print(\"After resizing:\",load_img_rz.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crop\n",
    "im = np.array(Image.open('data/images/fig5.png'))\n",
    "\n",
    "print(\"Before trimming:\",im.shape)\n",
    "\n",
    "im_trim = im[100:250,10:260]\n",
    "print(\"After trimming:\",im_trim.shape)\n",
    "\n",
    "Image.fromarray(im_trim).save('data/images/trim_kolala.png')\n",
    "\n",
    "#print(im_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/39382412/crop-center-portion-of-a-numpy-image\n",
    "def crop_center(img,cropx,cropy):\n",
    "    y,x,_ = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)    \n",
    "    return img[starty:starty+cropy,startx:startx+cropx]\n",
    "\n",
    "\n",
    "im = np.array(Image.open('data/images/fig5.png'))\n",
    "\n",
    "print(\"Before trimming:\",im.shape)\n",
    "\n",
    "\n",
    "\n",
    "im_trim = crop_center(im,4,6)\n",
    "print(\"After trimming:\",im_trim.shape)\n",
    "\n",
    "Image.fromarray(im_trim).save('data/images/trim_kolala.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train/test\n",
    "training_idx = np.random.randint(m, size= (m * 75)//100)\n",
    "test_idx = np.random.randint(m, size= (m * 25)//100)\n",
    "print(training_idx)\n",
    "print(test_idx)\n",
    "\n",
    "train_set_x, test_set_x = set_x[training_idx,:,:,:], set_x[test_idx,:,:,:]\n",
    "train_set_y, test_set_y = set_y[training_idx], set_y[test_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store inside HDF5 file\n",
    "file = h5py.File(\"data/myData.h5\", \"w\")\n",
    "file.create_dataset('set_x', data=set_x)\n",
    "file.create_dataset('set_y', data=set_y)\n",
    "file.close()\n",
    "\n",
    "# Read from the file\n",
    "file = h5py.File(\"data/myData.h5\", \"r\")\n",
    "set_x = file[\"set_x\"][:]\n",
    "set_y = file[\"set_y\"][:]\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier attempt\n",
    "# When result was a list of set_x and set_y\n",
    "# Uses list comprehension\n",
    "first = [x for (x,y) in result]\n",
    "set_y = np.concatenate(first,axis =0)\n",
    "\n",
    "second = [y for (x,y) in result]\n",
    "set_x = np.concatenate(second,axis =0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier attempt when result was a list of set_x and set_y\n",
    "# The entire result list was converted to array\n",
    "# But the conversion was very slow (and got stuck)\n",
    "result_array = np.array(result)\n",
    "\n",
    "set_y = result_array[:,0,:]\n",
    "set_y = set_y.reshape(150,).astype('int')\n",
    "print(set_y.shape)\n",
    "\n",
    "set_x_temp = result_array[:,1,:]\n",
    "print(set_x_temp.shape)\n",
    "#set_x_temp[1,1].shape\n",
    "set_x = []\n",
    "for i in range(set_x_temp.shape[0]):\n",
    "    for j in range(set_x_temp.shape[1]):\n",
    "        set_x.append(set_x_temp[i,j])\n",
    "    \n",
    "set_x = np.array(set_x)\n",
    "\n",
    "set_x.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DID NOT WORK\n",
    "#Try multithreading instead of multiprocessing\n",
    "\n",
    "#Gave this error :\n",
    "--> 393             self.figure.draw(self.renderer)\n",
    "    394             # A GUI class may be need to update a window using this draw, so\n",
    "    395             # don't forget to call the superclass.\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "#import threading as mp\n",
    "#import multiprocessing as mp\n",
    "#from multiprocessing.pool import ThreadPool \n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "\n",
    "def applyParallel(dfGrouped, func):\n",
    "    pool = ThreadPool(20)\n",
    "    ret_list = pool.map(func, [group for name, group in dfGrouped])\n",
    "    pool.Close()\n",
    "    pool.join()\n",
    "    return ret_list\n",
    "    \n",
    "data = df[df['Symbol'].isin(['A2M.AX','AGL.AX','ALL.AX'])]\n",
    "data_grouped = data.groupby('Symbol')\n",
    "result = applyParallel(data_grouped, inner_loop)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First attempt to created the set_x array\n",
    "# Loops through each Symbol at a time\n",
    "\n",
    "start_time = time.time()\n",
    "#data = df\n",
    "data = df[df['Symbol'].isin(['A2M.AX','AGL.AX'])]\n",
    "\n",
    "set_x = [] #np.empty(shape=(NO_OF_IMAGES,IMG_SIZE,IMG_SIZE,4))\n",
    "set_y = []\n",
    "data_grouped = data.groupby('Symbol')\n",
    "\n",
    "flag_alternate = 0\n",
    "\n",
    "for name,group in data_grouped:    \n",
    "    print(name + '  ' + str(group['Symbol'].count()))\n",
    "\n",
    "    loop_range=  (group['Symbol'].count()) -  (DATE_WINDOW) - 10\n",
    "    print(loop_range)\n",
    "    # No of images each group can have will be Row Count minus Window\n",
    "    #for i in range(loop_range):\n",
    "    for i in range(5):    \n",
    "        #print(\"Iter:\" + str(i))    \n",
    "        #Save the Target value of the last row as this will be the predicted value\n",
    "        #target_arr.append(data[-1:]['Target'].item())\n",
    "        set_y.append(group[-1:]['Target'].item())\n",
    "        \n",
    "        #Remove the last row and plot\n",
    "        group = group[:-1]\n",
    "        \n",
    "        \n",
    "        #Read the saved image and convert it into numpy after resizing\n",
    "        if flag_alternate == 0:\n",
    "            save_candlestick(group[-DATE_WINDOW:],name)\n",
    "            img_asNumpy = np.array(Image.open('data/temp_image0.png',mode='r').resize((IMG_SIZE,IMG_SIZE)))\n",
    "            flag_alternate = 1\n",
    "        else:\n",
    "            save_candlestick(group[-DATE_WINDOW:],1)\n",
    "            img_asNumpy = np.array(Image.open('data/temp_image1.png',mode='r').resize((IMG_SIZE,IMG_SIZE)))\n",
    "            flag_alternate = 0\n",
    "        \n",
    "        #image_without_alpha \n",
    "        img_asNumpy = img_asNumpy[:,:,:3]\n",
    "        set_x.append(img_asNumpy)    \n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instead of saving the image, just store directly into array\n",
    "    # In order to get the figure contents as RGB pixel values, the matplotlib.backend_bases.Renderer needs to first draw the contents of the canvas. You can do this by manually calling canvas.draw():\n",
    "    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "    from matplotlib.figure import Figure\n",
    "    fig = Figure()\n",
    "    canvas = FigureCanvas(fig)\n",
    "    canvas.draw()       # draw the canvas, cache the renderer\n",
    "    image = np.fromstring(canvas.tostring_rgb(), dtype='uint8')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    \n",
    "    set_x.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the Date column was not indexed\n",
    "min_date = df['Date'][df['Date'].index.min()]\n",
    "max_date = df['Date'][df['Date'].index.max()]\n",
    "df[df['Date'] > (max_date-timedelta(days=DATE_WINDOW))]\n",
    "df.loc[(max_date-timedelta(days=DATE_WINDOW)):]\n",
    "df.query('date == \"2020-01-01\"')\n",
    "\n",
    "#Since there is date time index\n",
    "min_date = df.index.min()\n",
    "max_date = df.index.max()\n",
    "data = df[max_date-timedelta(days=DATE_WINDOW):max_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#DO NOT RUN\n",
    "# Reshape X and Y\n",
    "set_x = set_x.reshape(set_x.shape[0],-1).T\n",
    "set_y = set_y.reshape(set_y.shape[0],-1).T\n",
    "\n",
    "print(set_x.shape)\n",
    "print(set_y.shape)\n",
    "\n",
    "\n",
    "#Shuffle X\n",
    "m = set_x.shape[1]                  # number of training examples\n",
    "permutation = list(np.random.permutation(m))\n",
    "set_x = set_x[:, permutation]\n",
    "    \n",
    "print(m)\n",
    "#Split train/test\n",
    "training_idx = np.random.randint(m, size= (m * 80)//100)\n",
    "test_idx = np.random.randint(m, size= (m * 20)//100)\n",
    "print(training_idx)\n",
    "print(test_idx)\n",
    "\n",
    "train_set_x, test_set_x = set_x[:,training_idx], set_x[:,test_idx]\n",
    "train_set_y, test_set_y = set_y[:,training_idx], set_y[:,test_idx]\n",
    "\n",
    "print(train_set_x.shape)\n",
    "print(train_set_y.shape)\n",
    "print(test_set_x.shape)\n",
    "print(test_set_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture \n",
    "## cell magic function `%%capture` blocks jupyter notebook output,\n",
    "## which is not needed here since the plot is saved to a file anyway:\n",
    "\n",
    "#plot_candlestick(df[max_date-timedelta(days=DATE_WINDOW):max_date])\n",
    "\n",
    "mc = mpf.make_marketcolors(up='g',down='r')\n",
    "s  = mpf.make_mpf_style(marketcolors=mc\n",
    "                        ,rc = {'xtick.major.bottom':False, 'ytick.major.left':False\n",
    "                               ,'xtick.major.size':0,'ytick.major.size':0\n",
    "                               ,'axes.labelsize' : 0\n",
    "                               ,'savefig.jpeg_quality' : 5\n",
    "                               ,'savefig.bbox':'tight'\n",
    "                               ,'patch.linewidth' : 0 #candle border\n",
    "                               ,'lines.linewidth' : 1.5 #wick width\n",
    "                               ,'axes.spines.left' :False #plot border\n",
    "                               ,'axes.spines.top' :False\n",
    "                               ,'axes.spines.bottom' :False\n",
    "                               ,'axes.spines.right' :False\n",
    "                               \n",
    "                              }\n",
    "                       )\n",
    "\n",
    "# First we set the kwargs that we will use for all of these examples:\n",
    "kwargs = dict(type='candle',volume=False,figratio=(5,5),figscale=1\n",
    "              ,savefig='data/images/fig'+ str(1) +'.png')\n",
    "mpf.plot(df[max_date-timedelta(days=DATE_WINDOW):max_date],**kwargs,style = s)\n",
    "    \n",
    "fig.canvas.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert plot to numpy array\n",
    "\n",
    "mc = mpf.make_marketcolors(up='g',down='r')\n",
    "s  = mpf.make_mpf_style(marketcolors=mc\n",
    "                        ,rc = {'xtick.major.bottom':False, 'ytick.major.left':False\n",
    "                               ,'xtick.major.size':0,'ytick.major.size':0\n",
    "                               ,'axes.labelsize' : 0\n",
    "                               ,'savefig.jpeg_quality' : 5\n",
    "                               ,'savefig.bbox':'tight'\n",
    "                               ,'patch.linewidth' : 0 #candle border\n",
    "                               ,'lines.linewidth' : 1.5 #wick width\n",
    "                               ,'axes.spines.left' :False #plot border\n",
    "                               ,'axes.spines.top' :False\n",
    "                               ,'axes.spines.bottom' :False\n",
    "                               ,'axes.spines.right' :False\n",
    "                               \n",
    "                              }\n",
    "                       )\n",
    "\n",
    "# First we set the kwargs that we will use for all of these examples:\n",
    "kwargs = dict(type='candle',volume=False,figratio=(5,5),figscale=1)\n",
    "\n",
    "mpf.plot(df[max_date-timedelta(days=DATE_WINDOW):max_date],**kwargs,style = s)\n",
    "\n",
    "#In order to get the figure contents as RGB pixel values, the matplotlib.backend_bases.Renderer needs to first draw the contents of the canvas. You can do this by manually calling canvas.draw():\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "fig = Figure()\n",
    "canvas = FigureCanvas(fig)\n",
    "\n",
    "\n",
    "canvas.draw()       # draw the canvas, cache the renderer\n",
    "\n",
    "image = np.fromstring(canvas.tostring_rgb(), dtype='uint8')\n",
    "\n",
    "\n",
    "#data = np.fromstring(fig.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "\n",
    "# If we haven't already shown or saved the plot, then we need to\n",
    "# draw the figure first...\n",
    "#fig.canvas.draw()\n",
    "\n",
    "# Now we can save it to a numpy array.\n",
    "#data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "#data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to get the figure contents as RGB pixel values, the matplotlib.backend_bases.Renderer needs to first draw the contents of the canvas. You can do this by manually calling canvas.draw():\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "fig = Figure()\n",
    "canvas = FigureCanvas(fig)\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.text(0.0,0.0,\"Test\", fontsize=45)\n",
    "ax.axis('off')\n",
    "\n",
    "canvas.draw()       # draw the canvas, cache the renderer\n",
    "\n",
    "image = np.fromstring(canvas.tostring_rgb(), dtype='uint8')\n",
    "data = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the dataset using the date index\n",
    "# but if there are missing dates then the no of records retrieved are not uniform\n",
    "target = np.zeros(NO_OF_IMAGES)\n",
    "max_date = df.index.max()\n",
    "\n",
    "for i in range(NO_OF_IMAGES):\n",
    "    #breakpoint()\n",
    "    print(\"Iter:\" + str(i))\n",
    "    data=df[(max_date-timedelta(days=DATE_WINDOW)):max_date]\n",
    "    print(data)\n",
    "    \n",
    "    #save_candlestick(data,i)\n",
    "    \n",
    "    #print(str(data.index.min()) + ' - ' + str(data.index.max()))\n",
    "    print(max_date)\n",
    "    varClose = data.iloc[-1]['Close']\n",
    "    varPrevClose= data.Close.shift(1).iloc[-1]\n",
    "    \n",
    "    print(varClose)\n",
    "    print(varPrevClose)\n",
    "    print(100 * ((varClose - varPrevClose)/varPrevClose))\n",
    "    target[i] = fn_setTarget(100 * ((varClose / varPrevClose) - 1))\n",
    "    print(target[i])\n",
    "    max_date = data.index.max()-timedelta(days=1)\n",
    "    print(max_date)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column using the apply function\n",
    "\n",
    "def fn_setTarget(val):\n",
    "    if val > UP_THRESHOLD_PCT:\n",
    "        out = 1\n",
    "    elif val < -DOWN_THRESHOLD_PCT:\n",
    "        out = -1\n",
    "    else:\n",
    "        out = 0\n",
    "    return out\n",
    "\n",
    "data\n",
    "data['Close_lag'] = 100* (data['Close'] - data['Close'].shift(1))/data['Close'].shift(1)\n",
    "data['Target'] = data.Close_lag.apply(fn_setTarget)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false \n",
    "\n",
    "# Plotly Saving to image is a pain\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import plotly\n",
    "\n",
    "fig = go.Figure(data=[go.Candlestick(x=df['date']\n",
    "                                     ,open=df['open']\n",
    "                                     ,high=df['high']\n",
    "                                     ,low=df['low']\n",
    "                                     ,close=df['close'])])\n",
    "fig.update_layout(xaxis_rangeslider_visible=False)\n",
    "fig.show()\n",
    "#Error while saving\n",
    "#fig.write_image(\"data/images/fig1.jpeg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1594171376362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}